Skillshare! - December 04
VIEW RECORDING - 77 mins (No highlights): https://fathom.video/share/8c9c4g7UbkTs-E6Prp5wXXNMSQxJvdqu

---

0:00 - Hannah Rosebraugh (savaslabs.com)
  It's real flames. Dangerous.

0:06 - Chris Russo (savaslabs.com)
  It's really, yeah, a friend bought me a screen for it that I have yet to put up because her dog comes around from time to time, but it's the same room.  I mean, obviously, I haven't been working from home much, but same room, but I figured it was a better angle.  Like the windows here. Used to be just facing that way, I guess. It's interesting, isn't it? Nice. You can pretend you're outside because you're looking into the wilderness instead of...

0:37 - Ian Curran (savaslabs.com)
  Well, I'm looking into a wall.

0:40 - Chris Russo (savaslabs.com)
  There is sunlight, but it's not that luxurious. Let's see. Okay. So I'm going to try to... My goal on, like, a marketing front...  Um. Yeah. Um. um Is to share, like, obviously be efficient in the way we share out some of this stuff.  So I'm bringing that up now because, like, I'm going to do this presentation, but I might try to, like, get little snippets of video, like, talking points to share on LinkedIn or whatever.  I don't know exactly how I'll do that yet, but I may, I'm like a little sniffly, so I might, like, just say something and then and say it again, you know, be a little bit more composed.  So, so that, that might be a better take and I'll, I'll plan to, like, look at my notes and slowly and then, anyway, just a little heads up on the presentation style and expectations there.  And I might sort of, like, slowly do some things that feel a little bit more awkward, but maybe I should, you know, maybe that should be a norm.  That sounds good. What should I do with my hands? Oh, man, I had a, I had a. Ron Burgundy reference in talking about AI will do whatever you tell it to do and you got to, you know, put whatever's on the teleprompter.  I had a joke in there somewhere, but I don't think it's made it through to the script. All right, this is not the first page.  Matt H joined. Let me screen share. I want to make sure I get the screen share looking right. So I want to share this full window.  So I've got some tabs open, but in general, I want to be in present mode. So that looks clean.  When I go from present, oops. Yeah, see, it does this weird thing. How did I solve that last time?  think I, oh, right. Put it on the big screen. Bear with me. You're not centered there. That's going to kill the vibe.

2:56 - Ian Curran (savaslabs.com)
  I know. Yeah. I don't know why I present.

2:59 - Chris Russo (savaslabs.com)
  I read. did fit to page, but I think it's like too narrow. So that's better, right? Yeah, that's much better.

3:06 - Ian Curran (savaslabs.com)
  So we're good there. And then present looks good.

3:09 - Chris Russo (savaslabs.com)
  I'm going to tab through tabs at times, but then go back to full screen for those just so we get the clean script.  Okay. I think we're good to get rolling. How's everyone doing? Any questions, any you want to start with? One more thing.  My notes. That's good. Happy. All right. I'm not going to show that. I am going to show that. Bye.  Bye. you. Okay. And one last thing that's there. All right. awkwardness of the presentation.

4:14 - Ian Curran (savaslabs.com)
  No, I thought this was all really smooth.

4:19 - Chris Russo (savaslabs.com)
  All right. Let's dive in. All right. So I think the sort of main initial thrust or when I said I wanted to do a Skillshare was mostly like demoing this, the secret Savas tool and a couple of the concepts or the main concept behind it from an AI perspective is this RAG architecture.  And it's been, it's been a, oh, by the way, please give me feedback real time. Just like I was saying, if I want to like, Hey.  You're touching your face too much or like you're off centered or whatever, you know, be critical. Tell me, you know, fix me.  I'm too fidgety or whatever. You know, you can't totally fix me, but like if I'm, you know, swaying over here and whatever, just like, hey, Chris, line, you know, I might reach.  Should we drop it in the chat or just. No, no, no. Say it. Well. Okay. Yeah. Say it. Cause I won't see the chat probably or I might miss it.  Okay. Thank you. All right. Thank you. My positioning correct here. Okay. So, so the inspiration or what I wanted to talk about, we are going to talk about is, was, it was an initial focus on RAG, but I also did a number of other things and I think we'll have time to kind of dive into them and see what that, you know, see what those conversations can yield.  Um, because once we touch a few of the concepts in RAG, it's not like a super, super complex thing.  Um, but. but. Um, but. But by show of hands, just for a couple of the concepts off the bat, like who feels like they know what RAG, and you can see it here, retrieval augmented generation, like means pretty well and could, I'm not going to put you on the spot to define it, but do you feel like confident in it or not?  Say if you feel confident in it, raise your hand. I thought Dan and you would do that. Okay, a couple of folks, and how about like, do you feel like you could define an AI agent well, and what that means?  Okay, a couple of folks. To be honest, for both, if you were on the clock a few weeks, I don't think, I might have like done this, but my conceptualization has grown quite a bit.  So, no, no shame in that, and we'll cover, we'll cover it all. Let me make sure. You can see my notes and you all at the same time.  All right. So did I say everything I wanted to say there? Vibe coding. but I don't know. Do I want to?  No, I'll save it. So the inspiration for this, what ended up being the secret savus tool, was kind of twofold.  Or using RAG with the secret savus tool is kind of twofold. One, for a long time, I wanted us to sort of derive intelligence from our Slack history.  We've got like a long running, many years old teamwork. And it used to be in like project management system before teamwork.  But it would be good to kind of pull in all the messages here and be able to query, you know, like, what were all the projects that were used this technology?  Or who were the clients that we worked with? And just like harvesting that data for usefulness. So that's kind of been like a background desire for a long time.  And then. And This opportunity with Riff where they've recently updated some of the things we've proposed to them months ago as they've come around and have updated their needs and desires in AI and kind of elevated what they're expecting there and specifically talked about expecting some sort of RAG implementation.  And we'll see if that ends up being the case, but those were kind of like two motivations that merged into the, all right, well, it be good to have a working prototype and for me to like deepen my understanding of how these systems and tools could be used to help our clients.  And I'll cover it in a little bit later, but I started going down some avenues with Riff's data specifically, ran us some dead ends and then unblocked those dead ends.  And so I sort of started like, what can we show Riff and we will eventually be doing that in the next couple of weeks, but started with the Secret Savas tool because it was something that I.  Built the matching, very simple algorithm years ago, and now it feels like, oh, we could do an AI update to that and write a blog post about it and share it.  Boom. So, like, oh, another question I going to ask. Like, who has used the tool, has signed in, generated GIFs, checked it out?  If you haven't done it yet, do it. And I have, I had another component that we'll look at. Okay, something most folks did.  That I was trying to get MGROC to work so I could share with you all because it's not my local host, but I couldn't, I couldn't get that going today, but you'll see.  Essentially, you know, wanting to, do I want to talk about, let's see, is this the time to, just make sure, inspiration twofold.  We're going to go to the app in the next slide. Okay. Essentially, We'll Bye. Wanting to, you know, building this tool to give you all and us the opportunity to kind of dig in and harvest the knowledge from Slack to sort of define what we might be like, the interests that we might have, and that sort of thing, and to be able to generate a gift, a gift list as ideas, right?  And that's the main function of what it does. I think of, in diving into like, maybe the two concepts that are, will be least familiar, certainly to anyone not on the engineering team, but also on the engineering team are, will be the vector database, right?  That's that middle piece, and the concept of embeddings, which are embedded into the vector database, and we're going to look at everything in a little bit more detail.  But I didn't come up with this analogy, guess who did AI, but I think that it's decently helpful to think about how this system works and how we'll use it for others.  So embeddings, and again, we'll look at some diagrams in a second. These ways to sort of turn language into math and store in a database in a means in which you can compare this sentence has a similar concept to this sentence, or this word has a similar concept to this word, or they're pretty divergent in their concept.  So those embeddings are literally just numbers in a whole long series of numbers, and this is like how LLMs work, that encode some semantic meaning of text.  And so that's what's happening with embeddings. So those are the senses, the smelling and the hearing. I don't know if you want to think about smelling your teammates and scent, but, you know, getting some understanding of people and their sentiments.  Based on what they've written, the vector database is where those embeddings get stored. So that's the memory. And then we're essentially sending all, you know, packaging.  The intelligence that we derive from those embeddings, storing it in the vector database, and then sending it to the LLM for some processing.  And we'll look at this very specifically of how we do it. So if that doesn't make total sense, don't worry about it.  It'll stick more in a moment. And advancing. Okay. So this is my, I don't know why these slides didn't work that way, but this is my cue to pull up the app.  All right. So I guess I'll just mention some quick design decisions with the app. Let me refresh it. Let's see.  I'll pull up. Can I offer another analogy for anyone?

12:39 - Dan Murphy (savaslabs.com)
  Yeah, please, please. An analogy I like to use for this is like, it's like taking an open book exam in college.  You're the LLM. You're synthesizing the knowledge. The books would be the database of knowledge that you can access. You've to pick which books you bring to the exam.  And like the embeddings is like the person. And writing the book, creating the book, like, you know, going out into the real world, synthesizing that information and making connections and putting it into a book.  Yeah, nice.

13:09 - Chris Russo (savaslabs.com)
  I like that. Yeah, please do. I mean, I always say this, but jump in and say, you know, anyone who has something to contribute.  And of course, questions, please ask. All right, so this might be better for me to do like this, so I can be looking at the camera.  But then for the PDF, I got to go back. You're still seeing the app, right? Cool. All right. I'll explain some, like, design decisions at a high level, and we will only dive into the things that are most relevant.  And, like, you can check out all this code. We can talk about whatever is helpful, you know, with the time that we have now and in the future.  But essentially, in creating this sort of like... It'd be interesting to do some sort of like, all right, we've got all this text about someone or that they've written in many cases over multiple years.  Of course, there was processing that I did to not like throw every possible message in there, but what are the ones that we're more likely to sort of share people's personalities and communication style?  I planned on like limiting these to the top few, but I was like, well, this is, you know, whatever, it might as well share more to get some sort of sense.  So I, and everything moved very quickly. This is definitely the most time that I spent like in cursor and vibe coding, code, I don't love vibe coding, using code generation tools.  And so there are some lessons learned on my end. We won't cover them because I think the team has a lot, but if there's a timer.  It has a lot of experience. so in some cases, like it built things really quickly and I wasn't totally sure of how it did it under the hood.  So I'll just focus the rag piece on the very end of the gift generation, but there were, some of what you see up here is using like Python libraries to analyze the Slack messages and come up with some sort of sentiment about someone's communication style.  I think at one point I was like, said something about humor and it pulled out its own. I think like on the whole, without tweaking and tuning this, I think for most people, it does a pretty good job.  Part of what I wanted to do was to embed like Slack messages to be like, all right, like take this example.  It says I'm, it's pretty confident. I'm, this is my profile that I'm into music and the like proof message embedded there is like nothing to do with real music.  was like a potential client that we were going to work with that we didn't. And so there's definitely plenty of fine tuning that would need to be done to like really hone this in, in a better way.  And like, but ultimately if you send a lot of messages and feed the LLM. It does a pretty good job, I think, at understanding you and what you want.  So I plan on like, let's just show me the top three interests, but when it generated a lot of messages, or went through a lot of messages, it created a lot.  And it's got this concept of how confident am I based on these messages. So if you look at mine, like soccer, I don't know, coffee, cooking, maybe biking is in there, hiking, video games is pretty off, I would say.  Okay, yoga, like some of them are not perfect, but others are like pretty, pretty good. And it did a pretty good job of pulling that out.  So I think if get a mix of like high quality and like medium, then the output ends up being pretty, still pretty high quality.  Or that was at least my, my take. And similar things, and I think for some of the generation, people will get messages embedded in these.  But similarly, it's sort of personality traits. Anyway, kind of. Kind of fun. And then the real rag piece here is these two opportunities to generate gift ideas.  And I wanted to use two different models to get a sense of what would open AI versus Gemini. Now, again, one of the design decisions I made was just like, I'm a little bit paranoid that maybe I could run up a big cost on this.  And so, like, I want to just like, I wanted everyone to be allowed to generate it once, but then it would sort of stick in the database.  And I think that's the experience that you all should have. But ultimately does a pretty good job of taking the intelligence from the messages.  And we'll look at how this, I have another view that will kind of tie this all together. And basically creating, like, packaging up what it's learned about me or you, and then sending that intelligence.  And that's just in the form of, like, looks like Chris. He's interested in these things. Here are the messages that he said, and sending that to the LLM and saying, give us five gift ideas for each.  And I did create a couple slides from now, a tool that will step into these a little deeper. But does that make sense?  What is going on there, roughly? Any questions, how that's working? We'll take a pause. Cool. All right. I'm going to come back to doing it over here.  And this. Present. Oops. Oops. Oh, my God. This is like when I click to that. What is going on?  Wait, is that OK? OK. cannot, What? Thank Yikes. That's still not fitting, right, isn't it? I think I need to do this again.  Fit. Is it vertical or horizontal? That looks good. We're back at the beginning. Okay, so to touch on one of the concepts that I mentioned about the embeddings, and I've got a couple of visuals that will try to tie this together.  As you can see, one of the more helpful ways I've seen it expressed and described is if you can imagine, like, what we're looking at here, the words mom and dad, once this algorithm embeds this in a vector database.  imagine how And it does the comparison of how similar are these two to each other. They're pretty similar concepts as opposed to like basketball or pizza, right?  And this idea of where they sort of land in this many, many dimensional space. I think one, I remember the one of the embeddings I use to one for one that's open AIs, like calling the API and getting back those embeddings and another one I rent locally.  And I think the local one has like some 1,500 dimensions. And I wanted to show this quickly just to, on a Chrome tab.  This is, just to give you an example, like this is a query that just, it's one of the messages that got embedded from Chelsea that just said off.  Probably not like the most. next podcast. you. Revelatory about a person's, you know, personality, but I'm trying to show you, like, this is what a vector database looks like, right?  And there, and I just showed you that, like, the first 20 embeddings, or first 20 digits, but each time it does an embedding, it embeds 1,500 of these.  So the point, the point in sort of showing that is that, and then let me come back to the graph, or the, the slide.  Is that the right one? Wait, where'd you go, window? So you can edit all these out, it's going to look great in the final video clip.  Where did my window go? Oh, is it there? The point in of showing that is those numbers correspond to...  Different regions within a many, many dimensional space. it's again, imagining it we're just a two dimensional space and this is not really how it works, but it's a way to visualize it.  These concepts of like mom and dad being close to each other is a way that when the embedding is doing, when the vector database is returning, it's searching, it's sort of saying like, okay, these two are close to each other.  They have similar meaning. These two are not so close to each other. They don't have similar meaning. We'll get into the details of how that's helpful in a second.  And maybe a more, I liked this, this feels like a little bit more relatable from a, this again is not how it actually works, right?  Because we're looking at two dimensions, like this gender dimension and age. But if you imagined that those were the only things that's encoding, you can see that grandfather and grand or grandfather and man are close to each other, both male.  And the age access, you know, they're up there in age, whereas boys, male, but down there in age, so you can see how, like, you might measure these distances.  So you put grandfather and man closer to each other than any of the other pair, or than the boy pairing or the woman pairing.  What if, where would, where would tween go? Oh, oh, . Let give it away. Where am I chew? You can give me a coordinate.  I think it would go between adult and child.

23:37 - Jayson Morse (savaslabs.com)
  Agreed.

23:40 - Chris Russo (savaslabs.com)
  Maybe. My bias is like, go ahead. Maybe age five, right in the middle. Right in the middle, yeah. Four, four and half-ish.  Yeah, I like that. How about grandmother? That's That's That's Nine and nine. Yeah, agreed. You get it. Okay. So the reality is like these, we don't really know, you know, when you're encoding something with 1500 dimensions, you can't explicitly say like this thing lives here, but the algorithms know how to do the matching of those, you know, it's obviously it's complicated.  We're not going to like dive into the depths of exactly how the algorithms work because I couldn't tell you that.  One misunderstanding that I had that it was helpful in doing this work, it cleared it up for me. I thought if we just ran embeddings on all of our messages, then I could sort of be like, tell me about Chris and what does he like and what does he like?  And then that's sort of like, I could just kind of, it would understand me and us. And so when I was first building it, I was like running all the embed.  And I was like, all right, now how do I figure me out? And there's been a question I've been asking for a long time.  So now we'll go, and that's not the case, and I'll demonstrate that a little bit more clearly here. This guy.  Okay, again, I want to pull this over so I can be looking at the camera. 30 minutes. It's taking longer than last run.  Full screen. All right. So the purpose, what we're looking at, once I got the V1 working, pushed it to the website, was like, all right, folks can use it.  It's like working. I wanted to, I sort of stamped that and said, all right, let me sort of, this is not really a V2.  This is just more visualizing what's going on under the hood. You're welcome. That's in a way that's sort of more relatable.  So I think was Maddy my next test case that worked well in my script? think so. All right. So what we're looking at is, and to come back to that misunderstanding, I thought the embedding just like sort of, I don't know, thinking retroactively, magically sort of just could tell you about a person.  But what, again, what I think you guys are understanding better now with the description is that, and maybe I'll make this a little bigger.  Let's see it better. Let's see Is you need to match a phrase or in order to understand what, if something someone has said is representative of an idea, you need to sort of match it against a phrase.  So the approach I took here was, you know, these 12 questions were generated by, you know, AI. didn't like very specifically call them out, but I was like, all right, I would like to learn.  I'd like to get a sense of people's heart. Hobbies and interests, things they're into, food, that sort of thing.  And so the way that we do that from a technical standpoint is to embed each of these phrases, right?  So the same thing you were just looking at with Chelsea's off. You embed things I do for fun in my free time, and then you get the embedding for that.  And then you are able to match that against something that somebody said in a message that is similarly semantic.  So if you said something like, every day after work, I enjoy playing basketball, those two phrases would be very semantically close to each other.  And that's kind of one of the main benefits of this vector database semantic is as opposed to needing to do like keyword search and always finding the, you know, a search has been for a long time.  They don't have to be the same words to know that they may be very similar in meeting. That's the kind of magic of it there.  So essentially, Matching these phrases against messages that people have written is the best way that we have of eliciting these kinds of, has someone given us indication about these in a message that they've written.  Does that make sense? Yeah, okay. All right, so section three here is going to test Maddy's messages. I hope there's nothing here you'd be embarrassed about.  It's all public channels, so it's on you. Against everything that we put into, that I put into and embedded, and I believe it was like 50,000-ish messages from like the 800,000 that we had had whittled down to use like mostly not project channels.  Try to weight internal channels more general, stuff like random things like that, that people would be more likely to give some indication of their interests and hobbies and that sort of thing.  So, you know. So what we're looking at here is, again, I mentioned using two different embedding libraries. I called this local one because I ran it on my machine.  You know, I was able to download the library and ran it and run those embeddings and store those locally.  then the OpenAI, we had to make requests to get the embedding, pull it back, and then store it in the database.  There's plenty of fine-tuning I would need to do here. For example, like all of the OpenAI distances, and again, distances that match of how close these two concepts are to each other, they're all a good amount higher than the local.  I don't understand why. But what we're looking at is ordered by, like, the top 10 messages of each of these categories that it thinks are most representative of someone saying a message about a thing they do in their free time.  So even looking at, this is great, the runner ducks make a, make a comeback. It was, it was funny to re-look at a lot of messages people sent in about.  So anyway, it was fun. So it does, on the whole, we won't look at them all in detail, I would say a pretty good job.  And there's a lot of text here to go through. So I'll kind of pan through it quickly, but maybe a couple of the last things I want to say.  Wait, reliving most embarrassing moments? Where is that? Oh, yeah. That's great. In a Skillshare. Real time. Wow. I couldn't have scripted that better.  You have, ultimately, we're going to take this and send it to the LLM. And you'll see that in a moment.  And these are the, like, these are the insights along with a prompt that says, come up with, you know, gift ideas for this person.  We have total control over the amount of messages. I think I defaulted to three for each. was like, give me the top three.  But again, I found the more information we sent to the LLM, the better. Because, you know, the rag and the sort of vector stuff is.  On RN to get really good, and LLM, if you give it good data, is going to give you good responses.  So I tended to just say, give it more. So, you know, a lot of them are, like, good, and then you'll find ones that are not terribly, I don't know.  But I think, again, that's sort of tuning on the vector side. Yeah, but, like, even, you know, food and drinks, like, a couple of top messages from the Savas Eats channels, like, a pretty good indication that it found it right.  Oh, and I think one thing I meant to say is part of the sort of magic of these vector databases is they can do this.  You're basically, like, matching the distance across every other kind of message out there. So they have one of their advantages is to, or one of the things it does really well is being able to efficiently match.  So it's, like, pulling the top ten. ten. Across all possible messages, and it does it pretty quickly to the top 10 matches, and it's not like storing those every single permutation of every match against every message.  anyhow, I don't fully understand how those algorithms work, but that's one of the strengths, is it can quickly scan many hundreds of thousands, millions of documents and get you a quick response to, like, of all that data, what is the message that's closest to what I'm looking for here?  Does that make sense? I'm going to keep scrolling down, stop me, ask questions. So there are 12 of these, I'm going go on 10, 11, 12.  I have notes for this. I think this will come up in a slide to come. This, one of the sort of lessons learned or improvements that I would have, I would make, I won't make or not anytime soon, but would consider making.  is this idea of chunking. So that's. That's when you determine, you can imagine, and it's a little bit different with Slack messages, but imagine we are trying to encode like long documents, PDFs, something that's much longer and many more words per unit.  It's up to you to sort of chunk. you can imagine it's like a 10-page PDF, and maybe I say I want to only take half a page at a time, so I might embed 20 different chunks, you know, the top half of the first page, bottom half.  20 different. So there's thought and there's sophistication that has to go into that to make sure it's sort of representative of what you want to cover, and we'll get to it in the Riff stuff in a minute.  But one of the sort of weaknesses of the initial approach was that every Slack message was just independently taken.  In other words, it wasn't aware of... ... Is this a reply to a question that somebody else asked, and therefore it might be out of context and the embedding isn't any the wiser?  And I have an example that's like a good, I mean, kind of, ah, Jesus, where is it? Where my notes at?  I'm on this one. I want to show, you know, I feel like I'll probably go a little long, just not, not terribly, but a little, just as a heads up.  This is the one. Okay. So this Slack message is the one where, that Zach specifically asks for interest and hobbies in the Secret Savas channel, which would be a very strong, you know, indication of where to harvest this information.  And when I first, and when Thank you. Let's pull this up. When I first pulled it up, there was, and it's still the case that the public site doesn't connect these, doesn't connect the fact that Zach says interests or hobbies at the outset and that everyone's reply is an indication of their interest and hobby.  So when I analyze, this is going to our, this is like a Slack link, right? And it can know what message that is.  So it goes and pulls the message that Zach wrote, interests and hobbies in the thread. And then it does this distance matching across both kinds of embeddings and says, all right, alone, that phrase is pretty close to a lot of these things, less close.  And once again, I don't fully understand why the OpenAI one is consistently lower. There's probably something that's not quite right there.  Anyhow. So, you know, pretty good matching across some of these that if someone says those words that they're probably talking about some of these things that are interest and hobby related.  And then I wanted to see, all right, if we had included all those, all the replies and see and gave it some context.  And so this is the thread with like clarity of, you know, this is a reply from Ben, is a reply from Zach on there.  There's just more context there. That's going to be a richer indication of what the conversation is about. Definitely for an LLM, to be honest, I'm not like 100% sure.  I expected this in running and attaching the replies. I expected it to like improve these scores. And some of them went down.  You could see like the open AI is like, all right, this seems closer, but not substantially. So there's something I still don't fully understand there, but.  So the point in demonstrating this was, dumb hats and stupid clothes, it's good, was to be able to kind of like envision what's going on under the hood, get some, like, see what the difference between including reply and you'd imagine in a real implementation, you'd have to be thoughtful about.  In fact, I was, this had come up, I had run into this issue and then I was talking with a friend who's been doing some rag stuff and was like, yeah, how did you do the chunking for the Slack messages?  And I was like, yeah, I didn't, I didn't, and it was something that they had run across too that you have to be thoughtful about how to put the replies together.  So, does that make sense? I think, analyze custom text, why did I do this? Let's, let's just do this for now.  For example, let's see if I remember, again, this is all trying to like visualize what's really going on under the hood.  So, I don't know if this is super helpful. But as I was trying to understand how distances worked and like how, you know, I wanted to be able to just try out some messages and see how they matched.  So interestingly here, kind of in the opposite, I used the exact phrase here. And so OpenAI says like, those are identical matches.  Those mean the same exact thing, which is what you'd expect. I do understand this other embedding. It's not the trailing space.  It's, I don't know that it's particularly interesting, but with this BGE, and I forget what those, what that acronym is for.  You basically have two modes to tell it to either look for documents or look for search text, which I don't fully understand the difference.  But when you run those embeddings, it says like, look for this text in a document and adds the phrase.  right. You're right. To it, that helps it do some like more intelligence on its end to either look for documents or, and I don't fully understand, but obviously like the distance is still really close.  And if I remembered the string, it's, you know, like look for text in a document that something, whatever the right string is there, this would come to zero.  Exactly. So anyway, you can see that with these excellent mat, you know, that it's very close. So this is just a way for me to familiarize myself more with like how it learned.  It's really hard to get things that are opposite as an interesting take. Like I hate peanut butter. I love peanut butter.  Those are actually going to be like very close to each other because you're talking about your preferences for peanut butter, which mine are very strong.  So coming up with things that are like very opposite, it was hard to do. But, you know, obviously the focus is mostly on things that are, oh yeah, let's, let's show it.  Yeah, that's right. I love peanut butter. I hate peanut butter. Let's compare those. Really diving deep on this. So distances that are not terribly far apart are sort of saying like these are pretty similar.  I think I wrote Texas. Oops. I always did that. Texas is a state. Yeah. I mean, those don't seem that similar to me, so I don't fully understand why that, but this is kind of more of what I would have expected.  And then maybe the most, most important or most helpful to drive home rag and what's going on here, just generate with Gemini.  don't think I've overused our free, whatever, limits, sunshine mood. Nice. Here it's showing you all the messages that it's.  Stuff's in, and remember there are 12, maybe only do 50, I forget why it's 50, I guess I might have expected 120, or 60, I don't know, 120, 10 per, and somewhere we've limited it to 50.  And here you're seeing, this is the prompt that is being sent to the LLM, so here's the RAG portion, right, saying like, here are some Slack messages from this person that talks about their interests and hobbies and likes and dislikes and values.  Frankly, we could have also put some of that, like, I've determined their communication style is this, maybe that's not that helpful for GIFs, I've determined their personality traits, some of that other, we could also, like, stuff to that in the top here if we felt really confident about it.  But we went with, or I went with, just showing messages that are revelatory about their interests, and then, you know, based on these messages, first summarize a person's interests, and then, you know, gives all the constraints.  Give us gift ideas. It's got to be $30 or fewer, that sort of thing. And then you get that LLM response, talking about their interest profile, and they get the ideas of runner duck stickers.  It's great. I don't think I saw that one yet. I love that seed variety pack for beginning. I mean, not a beginner gardener by any stretch, but I feel like these are like pretty good for Maddy.  I don't know, funny tech theme socks, maybe less so. She's a big fan of Supabase, though. So that's how it works under the hood.  I'm going pause for any questions. Does something not make sense? Is it interesting? Give me a, give me a, let's see, what are we on?  We're at 10 of 27 slides. think I can get, the rest of, like, spent most of the time there, so the rest of it will be quicker.  Any, any, anything. Who's got a pulse? Who might actually thought, some of you know. Use it. mean, has anyone gotten their gift influenced by this in any way?  I mean, like, part of this is just a little bit of an exercise. I don't think it's super, But maybe Hannah is the most helpful as the newest person to try to.  Who did you get, Hannah? Did you ask who I got?

43:21 - Hannah Rosebraugh (savaslabs.com)
  Because I'm going to say that on camera. The right response. It was a test. But I will say that had I not already taken care of this, it would have been very helpful.  Careful. Because if you sent it and it arrives. Nope. Didn't say I sent it. So I took care of it.  Okay. With this power. But it would have been nice to know. Cool. Let's go.

43:47 - Chris Russo (savaslabs.com)
  This is where it's. I got to switch windows again. Zach got me to. That's right. I know what you're doing.  See, I know what you're doing, and I don't like it. it. Name that movie. Anybody? Okay. So what do I want to say?  This, like notebook, if you can't tell, notebook LM made these slides, which I think is pretty cool. I didn't, I was happy to not spend the time on making slides and it, you know, you can like generate them from the context and the sources you have.  Didn't get everything right, but, you know, definitely helpful for the presentation. It costs more than this, but one of the, like I was, we had $10 worth of credits and I was like, let's see if I can, excuse me, do all this under that.  I had to rerun it. I think I ran all, I think I ran embeddings like 720 the first time when I didn't have it correct.  And then like in. And moving from local to remote somehow lost that first round of embedding. So it ran overnight, but it was only a few bucks.  And I think like all told, it was like five or six or seven dollars to do it all, which was like with learning and as I said, rerunning.  So it's just interesting to kind of pay attention to this, to how much you can get. And I don't know, I feel like I have a little more context now of what size of payload of data and what those might cost and that sort of thing.  Okay, what do want to say? Here we're sort of moving into the musings of a few other projects or a few other things I did quickly.  I'll kind of cover them quickly to see if anything is interesting. But I believe, yeah, there's not anything more on the rags, the rags, you know, the Secret Sava specific stuff.  Before pivoting a little bit and talking about some of these quickly, anything, any questions, any like, ooh, this would have, you know what, I did have a, I did somewhere have a, this in addition to the chunking thing, like we could have made this app better by analyzing other aspects of data that we have, like, you know, photos from random, for example, or, like, if we were recording our, like, weekend updates, those would be, like, really rich sources, but of course, also, like, privacy considerations there and that sort of thing.  I'm not just posting people's videos, you know, photos with their children over the internet, but anyway, those would be rich areas to kind of get real context, and I did one example in Gemini with a picture I had from Friendsgiving.  It was like, wow, really, like, really laid out the scene really well in, you know, in text format. You could even have it create scenarios and whatnot or so.  Anyway, no more. That ends the secret Savas rag, and I'll talk about a few other things. We'll see. Maybe it'll be 15 more minutes.  Try. Anything. You should have done this. I get it. I don't really understand vector data. What's a vector database?  What is an embedding? Why does that matter? What do we care? I'm going keep. All right. I'll keep going.  Clap. Yeah. You perfectly explained everything. Couldn't have been better. Great. Glad to hear it. Okay. What do I want to say about this?  I think the goal here is something Zach and I have been talking about in a number of realms, which is like, that's you.  That's right. This is Zach. is uh, of like organizing help. Helping organize our internal Google Docs for he and I, it's often like proposal templates and task order things such that we have like, we can create things more efficiently.  We sort of know all of the different kinds of tools or different kinds of like terminologies that we've used.  And we have sort of a bank that we can go to like, all right, this one makes sense. And this proposal, this condition makes sense.  So I think, let's see, I think the order of operations for me was seeing that Notebook LM can create slides and they're pretty good, but you can't really adjust them.  They're just flat image files. It's like, oh man, it'd really great to have some integration with like being able to create proposals more efficiently.  And it kind of went down a rabbit hole, connected it to Google Drive API, like, you know, wrote some scripts to do that and like had some pretty weak outputs on the proposal front.  But then I was like, all right, Google Docs as opposed to Google. Slides is like an easier avenue to get some, to sort of read from and recreate.  And so I think the sort of main thing here that this would have been an intractable problem for us to solve without AI.  It's not like doing any, you know, rag or anything sophisticated like that, but there were 100, I was able to like very quickly write a script, you know, with cursor to say like, go find all of the task orders.  Right. And help organize, help organize this certain subsection. At the end, create lots of terms and conditions. And we've done that, you know, not super, super consistently, or with like slightly different wording over time.  And basically like categorize them, group them so that we can make a bank out of them. And Zach and I've been talking about like, let's see if have time for that in weeks.  No one is going to go find all 160 documents. And like, that's just not going to be worth it.  For us as an organization, nor like would it have been worth writing a script from scratch that could do this.  So in this way, it was something that like pretty, you know, 20 minutes time was able to organize everything into like, all right, you've got these 80 terms, they're in these 10 groups, you slightly worded them differently in these ways and just worked.  And I think I had a slot or I have that somewhere, but I don't think it's worth like looking at too much, but we'll get that to you, Zach, and we'll be able to do it easily.  Drop that all in one Google Doc and be able to whittle that down and then we have our bank.  Yeah, I think this is the, essentially, again, it's not like using, it's not going out to an LLM to do processing on it, it's just using it to write a script efficiently that connects to our Google Drive API and this like, I guess, the tool, the underlying.  Algorithm that uses this jacquard, don't know if I'm saying that right, similarity, which did a really good job of mapping, you know, in this case, it's just matching one section of our terms and conditions, which are pretty common and pretty similar across, and only like a small subset of stuff, worked well.  What's the next? Next step from original, let's see. Another, how do I want to say this? Another project that I took on quickly that was sort of derived from just like running into quality issues in our copy on our website, or like broken links, there was like, that's what I found we had on a page, like maximize ROT instead of ROI.  It was kind of like painful, maximum. first page Why is that a rot? And then in another case, the AI tag, like on a blog post, AI, if you went to that tag, it was like a 403 because of a kind of a complicated views permission thing, which we didn't know.  So those are two things like, man, it'd be good to like scrape all of the things and get a report.  And of course, there's like scraping tools out there. There's tools that do some of this, but, you know, it was able to like pretty quickly script up something.  And I was like, oh, while we're at it, why don't we look at like SEO improvements we can make and if there's any WCAG issues and give me like grammar issues and that sort of thing.  And so, and part of the impetus for this, this crawler too, was, and I don't know if it's in a slide, yeah, it's a couple of slides, but I don't specifically talk about it in that slide, but was to also in preparation for this riff work.  And wanting to get their content, was trying to crawl their site and sort of pull down things and be able to do some sort of analysis there and sort of show that to them as well.  I ran into some dead ends on the Rift side because it was using a browser agent to visit sites, but I think it was moving, it was going too rapidly.  You have to sort of not trigger servers to be like, hey, is this a bot crawling my site? So I got blocked from Rift for a while and I couldn't load it in my browser.  And so it eventually learned from that and spaced things out a little bit more and I was able to crawl the Savas Labs site well.  So I think I have, yeah. I was able to crawl it, generated this massive report. What you're looking at is what's helpful to say about  I did, once again, did some, like, Python scripting as well as LLM work, and the Python scripting is the one that's, like, over-reported lots of errors because it didn't have the sort of dictionary of, all right, Savas is an okay word, right, all people's names, that sort of thing.  But the broken links, and, like, essentially everything that I sent to the LLM was, like, was solid. And, yeah, 763.9 misspellings in the library, but, like, the LLM-generated content was, like, all of it was correct.  These are all, like, proper misspellings on pages. I think, like, most painful one was something I wrote in 2016 where the title was Floridaia instead of Florida.  And then, like, you know, we won't go through much more, but the grammar feedback is pretty good, and the broken links feedback is pretty good.  So I think there's a case for, like, Like a crawler that can do lots of analyses and be doing so maybe in an agentic way over time, like, you know, and these tools are out there, but it was something that like with pretty quick feedback was able to generate a report with like meaningful, you know, meaningful things that ought to be fixed.  Go back. So that's the rot. Let me keep going because I didn't talk about that. So All right, maybe I'll go 10 minutes long.  I'll cap it at 10 minutes long. No more than 10 minutes after the hour. If you must drop at 5 Eastern, go for it.  So I mentioned running into a dead end with Riff. And let me see. I think I have something to show.  I think I I I Yeah, I ran into a dead end with Riff, but like returning to, I'm going to show this, restart my, I guess I'm probably not going to share publicly a Riff comment specifically, but make sure this is proof of concept files.  So in the case of Riff, after I had gotten blocked and learned from my ways, I didn't, I didn't rerun the crawler because with the info we had, we had like a database dump.  Sorry, let me start that over. One of the things I wanted, one of the things Natasha very explicitly asked for, and they have both in language from her and in the proposal, is to get better information that they can't currently access from like PDFs, PowerPoint files, and videos, right?  In other words, their, their current search, you can search. Text across stuff that's stored in the Drupal database, but they don't have any of those things.  There's no way to search those things. So I wanted to do a proof of concept that could grab all those file types and then process them, essentially show, and as far as I got, I didn't embed them or anything, but show that we could pull out the context from them and essentially turn that into text that we could then store in a database that we could search for semantically.  And so I think the markdown file here, and this is how cursor works, it does a bunch of scripting.  like, here's all the stuff we did, and here's a markdown file so you can see what I just did.  It shows, this is like a summary file that's essentially showing, I guess I should be looking, I should be looking here.  Oh boy, how do I maximize you now? Oh, we'll just go with it. Yola. Oh, that's the full concept.  I want a proof of concept. You can, you know, you can look at this in more detail later, but it essentially processed, successfully processed, like, 30 PDFs and 12 PowerPoints.  And sort of, like, pulls out the metadata, as well as you can sort of see on slide two, they talk about this.  On slide four, it talks about this. So sort of, as we were talking about chunking earlier, you could imagine chunking per slide.  So, you know, the output for the user experience could be like, hey, you asked about graphemes and phonemes. Those are complicated things.  These little kids are learning. Oh, Jesus. Proof of concept. Go back. Anyway, you could search, you could query, and it's, hey, I'm looking for any resources you have about phonies.  God, I hope I'm saying that right. And the search ought to be able to, with what we have access to, say, actually, on this resource, this PDF on slide two, we talk about that, right?  So that's a way to get access to those and very pointed access to those resources that they currently have no, there's no way to search for, you would just have to know.  And then on, so that's, that was like PDFs and PowerPoints, like, worked pretty much immediately. And then on YouTube, or sorry, the video side of things, there's like three use cases.  One, anything that's a YouTube video is very easy to get the transcripts from, as you, as this sort of shows here.  They also switch, I think they switched to Vimeo at some point, sometime in the last couple years, because it seems to be the more, more, recent content in Vimeo, I could be wrong about that, but that's what it looked like in the database.  They embed some of the Vimeo videos on their website, and when they do that, there's actually like a transcript in the HTML that you can easily pull with timing and all that, so that's really helpful.  And then in the other case where Vimeo was, you know, we could grab the link and go off to like the Vimeo URL, the script was having a hard time pulling the transcript from that, but I think the solution there is like if we had their account and could do some API, could access the Vimeo API, would probably easily get the transcript.  So anyway, and this proof of concept is able to pull out PDFs, PowerPoints, and like two of the three use cases for videos, and then one of these like full context markdowns, it's got like, shows you all the things that you can see, but just as like a preview, oof, God, don't touch it, Chris, don't double click.  So anyway, that all just worked, or with some... With some massaging, now we're in a position in the next couple of weeks to build some proof of concept of like, hey, this is what you've asked for.  Now we can show you, we can search against this PDF, you know, search against looking for this thing, it's going to land in this PDF.  And we just discussed that's, I'll be doing that at some point in the next couple of weeks before we submit to Riff, where I just lost my, you guys are doing great.  Oh, it's probably over here, multiple desktops, come on, oh boy. Jesus Christ. Control, there we go, all right. Stop here, window, boom.  Riff's dark data, yeah, interesting choices for notebook LM, where they decide to call whatever they, call. Thank Thank And yeah, whatever, we don't have to go through all the details of the, I'll go present mode.  Anyway, that was successful. I'm going to skip this. It could be a topic for another day. It was a little bit of like, just, I do want us to be, I want us to be having conversations about the future of things and where everything's going.  And I, I at least found out to say very quickly, some of working with our own website with the new, like a new, with CodeGen tools, and being able to really spin things up quickly.  There's like some slowdown to being like, actually, you got to get this in the database. And there's like a whole component architecture to this.  And so I think the sort of use case of a future of being able to spin things up quickly and making that more and more accessible to, you don't have to be an engineer to, to do a lot of these things is like an, like, what is the future of quick, quickly editable sites and like how important is a CMS?  And I would, I would argue the, the, Value of a CMS is like going down a bit, and just for a little bit more context for folks who haven't been around for 10 years, we used to have a Jekyll, a Jekyll is a Ruby CMS that's like written in no database, blog posts would be written in markdown files and you'd push up in GitHub and that would deploy to the live site.  And we switched, you know, once we hired like marketing people who couldn't push to push PRs, but I think there are some, let's see, with a newer landscape, there's like a stronger case for that former workflow working for a wider array of people, and there's of course like trade-offs and drawbacks and all that.  So anyway, I think it could be an interesting conversation for the dev team, especially going forward, which is something I gave some thought to.  I want to get to, maybe we'll come back to that if we have time. But I'm going to keep rolling.  It's just sharing that we're going to be pushing out more content around AI. And this is like going to have the newsletter be sort of practical AI focused.  We're promoting a new like AI readiness and opportunity assessment and just trying to fit in that realm of like a lot of people are trying to figure out what to do and not overpromising that can fix all the things.  But like you need to be strategic in how you apply it. So I'm going to be pushing out a lot more content.  All right, this is the agent. This is the last thing. This is the thing I did last night that I wanted to I was working to set up an MCP server.  This is something. Don't worry about it. Something Dan has talked about in the past, but think of it as a means for an LLM to efficiently talk to other tools and scripts.  And so that's the purpose of an MCP server to help agents talk between LLMs and use tools and go do things in the world.  I was really unsuccessful. Cursor was. Really unsuccessful. Nothing wrong with me. And it's funny. was like the first time where I was like, what the 's wrong with you?  Like, come on, Kurt, get it, you know? It's funny to have that experience of this thing that can just build so much so quickly and, you know, anyway.  So I was trying to do a Drupal use case and failed and wasn't able to set up an MCP server last night.  And then I was like, I didn't give up. I pivoted. Here's where, like, here's where even my own understanding in the last week of like how I would describe an AI agent has changed.  I think like there's this, I've seen this out there a little bit. I don't know how common knowledge it is like across the board, but this concept of like react or AI agents, I don't know, it's an acronym, meaning like the agent has to be able to reason.  So you're not like explicitly describing everything it should do and then take an action. Right. And so the whole idea is.  that this agent should be able to autonomously, you know, with guardrails and constraints, take some actions without human intervention.  And the like reasoning is the way where it talks to an LLM and says, OK, I've got this output.  What do you think about that? Right back and forth. So like, I don't know, even with like looking into the MCP server and setting this up and like thinking of use cases, was like, I still don't know if I am creating this in the right way.  And this sort of like reasoning plus take an action, like crystallize things a little bit. So what I did set up, oh, , did it work?  Internal channel. It went a little bit longer. Yeah, man. Oh, yeah, Matt responded to it again. Wait, were you seeing it last night?  Were you up at that time? I was deleting those messages. No, it just, it wants me to go again.

1:06:53 - Matt Hisamoto (savaslabs.com)
  All right. Yeah, so a little improvement there.

1:06:58 - Chris Russo (savaslabs.com)
  Okay. So. What I did set up, sorry, coming back to the, yikes, I'm really struggling with these three different, what I wanted to set up was like, all right, I do this manual task each week of like looking who went in a prior weeks, you know, I'm sure sometimes you see that it doesn't happen until like right before the call or an hour before, whatever.  And I want to be able to delegate that duty to an agent of like looking in recent past and generate, generate the list of who should go, Matt points out that it's still got a little bit of a bug in it.  But what, so I did create, the stack is like a getting access to Airtable where we have names and people stored and if they're active, like for folks who are going out on maternity leave.  Like they will become an active, right? And so they won't be part of the message. I've already checked that off for Chelsea, should have a month ago or more.  But so it reads that, it has access to Slack to look back in the channel history, and then the sort of intelligence that I've given it is like, hey, look at recent history and try to balance it and come up with an order that makes sense.  And clearly there's some like more refining of the, a little bit, but I did find like with slight, slight prompting, it got like a good amount better very quickly.  And then it, and it, if it looks good, it makes the decision and it posts the Slack. So it's, it's doing the reasoning, it's talking to the LLM and it's taking the action.  So it was like a low stakes proof of concept of an agent and the, the server that the secret Savas is on is like, has a cron job, an automated job that just runs once a week and sends out that message to the MCP server.  So that was like gratifying to get going and have a, have an example of a agent use case and.  Oh, that's like two more slides. Maybe just I'll leave this like a couple of ideas here for future fodder thoughts.  One of the things I think I'll take action on too is Matt and Lou, or Matt and Lou will remember, Lou had to drop, but when we were working on Stryver, and it looks like I accidentally capitalized the T, that made it its way through.  There was, there were a number of things we were doing, but one of the things was like, trying to vet document, like, if we're running a kickoff, it should have these 10 rules of things we make sure we talk about that sort of putting some structure to what the document should have, and just running some tests against like, okay, does it have it?  If it doesn't have that, let me give you some like guidance on, you know, what it should and potential ways to update.  So I think we'll, we'll make some, probably build out some things there that can help kind of offload. Zach, I'm sure you can imagine like.  Thank you. So that could be helpful on proposals or task orders or like something to just run some quick checks against.  Yeah, guess that was that meeting order. Well, it's interesting, the order in which it created this. I think, yeah, this is the last slide, last concept.  another, you know, having the, another thing I'd like to dive into is, having an agent try to do, like, look into what, like, running a Drupal security or a WordPress update could look like and auto-posting to a staging site as another means of something that there's, you know, ability to, oh, I wanted to make sure I have some notes on this.  Links if I need, um, know, that's not, uh, well, that's Essentially, how, how, Trying an agent be able to see that an update is ready, make some moves on that update, maybe push it to a staging server, run some analyses and test visual regression.  Has anything changed? We don't want it to change. Can we still, for our website, for example, does the HubSpot form still work?  Is the HTML all the same on these most important pages? And if so, like that looks like a good, a good update.  Let me like push that to a PR in GitHub and or stage that on a staging URL. And obviously there's different, you know, there's different servers and where some of our sites are.  But I would like to kind of proof of concept that as like something that would have some organizational value, you know, takes us time to do those things.  Some of it should be automatable. We even pay for an outside service to do some of these for us.  So like has some financial return if it was actually successful. Anyway, that's an agent in like a slightly higher value than the sort of Slack example that I'm thinking about and want to do some testing on.  That's all I got. I think that's where it ended. Blueprint. Yeah, I don't know about these diagrams that don't, you know, it's a quick.  All right, so we're in 15 minutes. Any questions? I hope I wasn't doing weird things. No one interrupted me.
  ACTION ITEM: Share GitHub links for Secret Savas, Drive T&Cs, site crawler, Slack agent w/ team - WATCH: https://fathom.video/share/8c9c4g7UbkTs-E6Prp5wXXNMSQxJvdqu?timestamp=4351.9999  It means I should have had my presentation should have been perfect all along. Any thoughts, follow-ups, questions, clarifications, things I got wrong?  Where should we take this? I do have the code for free of these things that I push to GitHub, so I'll like share, you know, obviously if people want to get in touch and get some experience with any of it and help support some of these things.  Going forward will certainly be opportunity. So I'll share that after you unmute. did, Maddy. I'm just going to say, thanks.

1:13:06 - Maddy Closs (savaslabs.com)
  You're welcome.

1:13:09 - Dan Murphy (savaslabs.com)
  Well done.

1:13:13 - Chris Russo (savaslabs.com)
  I feel like a lot of these things you can't really understand as they start getting into the weeds and trying to use them.

1:13:19 - Dan Murphy (savaslabs.com)
  Yeah, yeah.

1:13:20 - Chris Russo (savaslabs.com)
  Like having the, you know, the working examples are like, I'm happy to stage that V2 piece, whatever, like, does, does feel, good, was a good learning tool.  All right. Oh. I wonder if cursor will, can cursor produce a dad joke? Let's see how it does. What do you think, cursor?  No. That's great. Yeah. You know, I mean, last, last additional comment I'll make is just like. Yeah. I think maybe one of the bigger takeaways, not everything is AI, obviously the RAG stuff is, but a lot of this is just being able to build stuff faster, and the economics of that has opened up again.  I would never touch that Google Drive tool, it just would not have been worth spending 30 hours writing a script, but if you can do it in one, that makes sense.  So I do think like us figuring out opportunities and being thoughtful about, and even like pitching new clients, like we know that building upon existing infrastructure is harder, and you can't just vibe code that, but like are there opportunities for us to build smaller, quicker, iterative things that you can build really quickly?  Like I'm thinking about with Riff and Grogo, just like, let's make, like taking a step back from just their apps and websites and focus, okay, what are their business goals?  And like, can we, with Grogo, can we actually be thoughtful about creating any sort of like agentic or otherwise experience that can like nudge potential?  Subscribers, like, you know, getting, and like Riff has like listed out a bunch of specific KPIs, not just like we want this search experience, but like what, what can we think about doing that we can build really quickly from scratch that might help like nudge some of those things.  And I do think like that's, you know, where like there's the hype of what AI is going to solve all the world's problems.  It does, it has certainly like unlocked this new opportunity to kind of build from scratch things that could, these kind of more microservice-y things that could provide more value.  So I like want to be thinking that way and encourage y'all to as well as where it is like coming into, but it's great.  I loved it, was great. That one time I got mad at it, I mean, spent a lot of, you all were enjoying your Thanksgiving as one should be, and like a normal person with family and friends and stuff.  I was like vibe coding the  out of it for hours on end. It was good. It was fun. It's fun to have that time and kind of like, I don't know, it's definitely, it's like inspiring and, you know, and energizing to be able to do new things.  All right. Give me an excellent dad joke to close out this amazing meeting. We all agree. I don't know if cursor will do this.  Yeah. That's pretty good. I like it. I don't remember this one. Why don't scientists trust atoms? I want to make sure I can close it out.  Is this the right, is this the pip? Anybody? Why don't scientists trust atoms? Because they make up everything. Have a great rest of your day.
